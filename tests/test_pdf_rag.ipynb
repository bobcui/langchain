{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "vector_store = None\n",
    "llm = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_vector_store() -> VectorStore:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vector_store = InMemoryVectorStore(embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def load_pdf_into_vector(file_path) -> str:\n",
    "    \"\"\"Clear the current PDF data and reset the vector store.\"\"\"\n",
    "    global vector_store\n",
    "    try:\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pdf = loader.load()\n",
    "        _ = vector_store.add_documents(documents=pdf)\n",
    "        return \"Pdf is loaded into vector.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error loading PDF: {e}\"\n",
    "\n",
    "@tool\n",
    "def clear_pdf_vector_store() -> str:\n",
    "    \"\"\"Clear the current PDF data and reset the vector store.\"\"\"\n",
    "    global vector_store\n",
    "    vector_store = get_new_vector_store()\n",
    "    return \"Vector store cleared.\"\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def query_pdf_with_vector(query: str) -> tuple[str, list]:\n",
    "    \"\"\"Retrieve information related to a query from the loaded PDF.\"\"\"\n",
    "    global vector_store\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.tools import Tool\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# tools = ToolNode[\n",
    "#     Tool(\n",
    "#         name = \"load_pdf\",\n",
    "#         func = lambda input: load_pdf_into_vector(input),\n",
    "#         description = \"Use this tool when the user asks to load or upload a PDF file. Input should be the file url, like 'https://arxiv.org/pdf/2503.00085'.\"\n",
    "#     ),\n",
    "#     Tool(\n",
    "#         name = \"clear_pdf\",\n",
    "#         func = lambda _: clear_pdf_vector_store(),\n",
    "#         description = \"Clear the PDFs or data from vector\",\n",
    "#     ),\n",
    "#     Tool(\n",
    "#         name = \"query_pdf\",\n",
    "#         func = lambda input: query_pdf_with_vector(input),\n",
    "#         description = \"Query the PDFs with a question\",\n",
    "#     )\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_or_respond(state: MessagesState) -> dict:\n",
    "    \"\"\"Let GPT decide whether to respond or call a tool.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([load_pdf_into_vector, clear_pdf_vector_store, query_pdf_with_vector])\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def generate(state: MessagesState) -> dict:\n",
    "    \"\"\"Generate GPT response using the output of tools.\"\"\"\n",
    "    tool_outputs = [msg for msg in reversed(state[\"messages\"]) if isinstance(msg, ToolMessage)]\n",
    "    docs_content = \"\\n\\n\".join(msg.content for msg in reversed(tool_outputs))\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "        content=(\n",
    "            \"You are an assistant for answering questions based on retrieved context. \"\n",
    "            \"If you don't know the answer, say so. Be concise (max 3 sentences).\\n\\n\"\n",
    "            f\"{docs_content}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    convo = [\n",
    "        msg for msg in state[\"messages\"]\n",
    "        if isinstance(msg, (HumanMessage, AIMessage)) and not getattr(msg, \"tool_calls\", False)\n",
    "    ]\n",
    "\n",
    "    final_response = llm.invoke([system_message] + convo)\n",
    "    return {\"messages\": [final_response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v_/495_mfbd4rv76fxlb3w_3hs80000gn/T/ipykernel_79896/2600605896.py:4: UserWarning: WARNING! store is not default parameter.\n",
      "                store was transferred to model_kwargs.\n",
      "                Please confirm that store is what you intended.\n",
      "  llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", store=True)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    global llm\n",
    "    llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\", store=True)\n",
    "\n",
    "    global vector_store\n",
    "    vector_store = get_new_vector_store()\n",
    "\n",
    "    tools = ToolNode([\n",
    "        load_pdf_into_vector,\n",
    "        clear_pdf_vector_store,\n",
    "        query_pdf_with_vector\n",
    "    ])\n",
    "\n",
    "    graph = (\n",
    "        StateGraph(MessagesState)\n",
    "        .add_node(\"query_or_respond\", query_or_respond)\n",
    "        .add_node(\"tools\", tools)\n",
    "        .add_node(\"generate\", generate)\n",
    "        .set_entry_point(\"query_or_respond\")\n",
    "        .add_conditional_edges(\"query_or_respond\", tools_condition, {\n",
    "            \"tools\": \"tools\",\n",
    "            END: END\n",
    "        })\n",
    "        .add_edge(\"tools\", \"generate\")\n",
    "        .add_edge(\"generate\", END)\n",
    "        .compile()\n",
    "    )\n",
    "\n",
    "    print(\"ðŸ¤– LangGraph PDF Chatbot\")\n",
    "    print(\"Say something like:\")\n",
    "    print(\" - 'Load this PDF: https://example.com/doc.pdf'\")\n",
    "    print(\" - 'What does the PDF say about climate change?'\")\n",
    "    print(\" - 'Clear everything'\")\n",
    "    print(\" - Type 'exit' to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"ðŸ‘‹ Bye!\")\n",
    "            break\n",
    "\n",
    "        result = graph.invoke({\"messages\": [HumanMessage(content=user_input)]})\n",
    "        for msg in result[\"messages\"]:\n",
    "            if isinstance(msg, AIMessage):\n",
    "                print(f\"\\nðŸ¤– Bot: {msg.content}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
